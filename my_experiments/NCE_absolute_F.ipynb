{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Contrastive Estimation to Calculate Absolute Free Energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import scipy.integrate as integrate\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Muller_potential(beta, x): #x must be type tensor\n",
    "    A = (-200., -100., -170., 15.)\n",
    "    b = (0., 0., 11., 0.6)    \n",
    "    ac = (x.new_tensor([-1.0, -10.0]),\n",
    "          x.new_tensor([-1.0, -10.0]),\n",
    "          x.new_tensor([-6.5, -6.5]),\n",
    "          x.new_tensor([0.7, 0.7]))\n",
    "    \n",
    "    x0 = (x.new_tensor([ 1.0, 0.0]),\n",
    "          x.new_tensor([ 0.0, 0.5]),\n",
    "          x.new_tensor([-0.5, 1.5]),\n",
    "          x.new_tensor([-1.0, 1.0]))\n",
    "    \n",
    "    U = 0    \n",
    "    for i in range(4):\n",
    "        diff = x - x0[i]\n",
    "        U = U + A[i]*torch.exp(torch.sum(ac[i]*diff**2, -1) + b[i]*torch.prod(diff, -1))\n",
    "\n",
    "    return beta*U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Muller_potential_point_nobeta(r):\n",
    "    \"\"\"\n",
    "    Computes the Muller potential at a point r = (x, y). r does not have to be a tensor\n",
    "    \"\"\"\n",
    "    x = r[0]\n",
    "    y = r[1]\n",
    "    A = (-200., -100., -170., 15.)\n",
    "    a = (-1, -1, -6.5, 0.7)\n",
    "    b = (0., 0., 11., 0.6) \n",
    "    c = (-10, -10, -6.5, 0.7)\n",
    "    x0 = (1, 0, -0.5, -1)\n",
    "    y0 = (0, 0.5, 1.5, 1)\n",
    "\n",
    "    result = 0\n",
    "    for k in range(4):\n",
    "        result += A[k]*np.exp(a[k]*(x-x0[k])**2 + b[k]*(x-x0[k])*(y-y0[k])+ c[k]*(y-y0[k])**2)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NCE, self).__init__()\n",
    "        self.U_x = nn.Sequential(\n",
    "          nn.Linear(2, 20),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(20, 20),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(20, 20),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(20, 1),\n",
    "        )\n",
    "        self.beta = 0.05 \n",
    "        self.muller_energy = compute_Muller_potential\n",
    "\n",
    "    def forward(self, x):\n",
    "        #return -self.beta*self.U_x(x) \n",
    "        return -self.beta*self.U_x(x)\n",
    "        \n",
    "    def ln_p_m(self, x): \n",
    "        #return -self.beta*self.U_x(x) \n",
    "        return -self.beta*self.U_x(x) #-self.muller_energy(self.beta, x) \n",
    "    \n",
    "    def ln_p_n(self, noise_samples, noise_beta):\n",
    "        return -self.muller_energy(noise_beta, noise_samples)\n",
    "    \n",
    "    def G_x_theta(self, x, noise_beta):\n",
    "        return self.ln_p_m(x) - self.ln_p_n(x, noise_beta)\n",
    "\n",
    "    def h_x_theta(self, x, noise_beta):\n",
    "        return torch.sigmoid(self.G_x_theta(x, noise_beta))\n",
    "    \n",
    "    def loss(self, X_true, Y_true, noise_beta=0.05):\n",
    "        #Y_true is a huge misnomer. Y_true is X_noise.\n",
    "        T = X_true.size()[0] + Y_true.size()[0]\n",
    "        J_T_vec = torch.log(self.h_x_theta(X_true, noise_beta)) + torch.log(1 - self.h_x_theta(Y_true, noise_beta)).to(device)\n",
    "        return -(1/(2*T))*torch.sum(J_T_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Create Basin1 and Basin2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bounds of the total area, but this is kind of meaningless\n",
    "x1_min, x1_max = -1.5, 1\n",
    "x2_min, x2_max = -0.5, 2.0\n",
    "\n",
    "Abounds = [[-1.5, 0], [0.55, 2]] #basin 1 bounds. So -1.5 < x < 0, 0.55 < y < 2.\n",
    "Bbounds = [[-0.8, 1], [-0.5, 0.8]] #basin 2 bounds. So -0.8 < x < 1, -0.5 < y < 0.8.\n",
    "\n",
    "d = os.path.abspath('')\n",
    "beta = 0.05\n",
    "\n",
    "with open('Asamples_beta_{:.3f}.pkl'.format(beta), 'rb') as file_handle:\n",
    "    dataA = pickle.load(file_handle)\n",
    "\n",
    "xsamplesA = dataA['x_record']\n",
    "betasA = dataA['beta_lst']\n",
    "\n",
    "with open('Bsamples_beta_{:.3f}.pkl'.format(beta), 'rb') as file_handle:\n",
    "    dataB = pickle.load(file_handle)\n",
    "\n",
    "xsamplesB = dataA['x_record']\n",
    "betasB = dataA['beta_lst']\n",
    "\n",
    "betas = betasA\n",
    "\n",
    "data_dictionaryA, data_dictionaryB = dict(), dict()\n",
    "\n",
    "counter = 0\n",
    "for beta in betasA:\n",
    "    data_dictionaryA[float(beta)] = xsamplesA[:, counter, :]\n",
    "    data_dictionaryB[float(beta)] = xsamplesB[:, counter, :]\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of defined betas: [0.0010, 0.0064, 0.0119, 0.0173, 0.0228, 0.0282, 0.0337, 0.0391, 0.0446, 0.0500]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model on Region A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter, Beta = 0.001 0\n",
      "Iter, Beta = 0.0064444444444444445 0\n",
      "Iter, Beta = 0.01188888888888889 0\n",
      "Iter, Beta = 0.017333333333333333 0\n",
      "Iter, Beta = 0.001 1\n",
      "Iter, Beta = 0.0064444444444444445 1\n",
      "Iter, Beta = 0.01188888888888889 1\n",
      "Iter, Beta = 0.017333333333333333 1\n",
      "Iter, Beta = 0.001 2\n",
      "Iter, Beta = 0.0064444444444444445 2\n",
      "Iter, Beta = 0.01188888888888889 2\n",
      "Iter, Beta = 0.017333333333333333 2\n",
      "Iter, Beta = 0.001 3\n",
      "Iter, Beta = 0.0064444444444444445 3\n",
      "Iter, Beta = 0.01188888888888889 3\n",
      "Iter, Beta = 0.017333333333333333 3\n",
      "Iter, Beta = 0.001 4\n",
      "Iter, Beta = 0.0064444444444444445 4\n",
      "Iter, Beta = 0.01188888888888889 4\n",
      "Iter, Beta = 0.017333333333333333 4\n",
      "Iter, Beta = 0.001 5\n",
      "Iter, Beta = 0.0064444444444444445 5\n",
      "Iter, Beta = 0.01188888888888889 5\n",
      "Iter, Beta = 0.017333333333333333 5\n",
      "Iter, Beta = 0.001 6\n",
      "Iter, Beta = 0.0064444444444444445 6\n",
      "Iter, Beta = 0.01188888888888889 6\n",
      "Iter, Beta = 0.017333333333333333 6\n",
      "Iter, Beta = 0.001 7\n",
      "Iter, Beta = 0.0064444444444444445 7\n",
      "Iter, Beta = 0.01188888888888889 7\n",
      "Iter, Beta = 0.017333333333333333 7\n",
      "Iter, Beta = 0.001 8\n",
      "Iter, Beta = 0.0064444444444444445 8\n",
      "Iter, Beta = 0.01188888888888889 8\n",
      "Iter, Beta = 0.017333333333333333 8\n",
      "Iter, Beta = 0.001 9\n",
      "Iter, Beta = 0.0064444444444444445 9\n",
      "Iter, Beta = 0.01188888888888889 9\n",
      "Iter, Beta = 0.017333333333333333 9\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "modelA = NCE().to(device)\n",
    "optimizerA = optim.Adam(modelA.parameters(), lr=0.5*1e-3)\n",
    "\n",
    "def train_helper(model, optimizer, noise_samples, true_samples, btrue, bnoise):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.loss(true_samples, noise_samples, bnoise)\n",
    "    loss.backward()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "    optimizer.step()\n",
    "   \n",
    "\n",
    "def train_main(model, optimizer, betas, data_dictionary, btrue, iters=10):\n",
    "    true_samples = torch.tensor(data_dictionary[btrue]).to(device)\n",
    "    for epoch in range(0, iters, 1):\n",
    "        for beta in betas:\n",
    "            noise_beta = float(beta)\n",
    "            batchsize= 150 #150\n",
    "            if noise_beta < 0.02: \n",
    "                print(\"Iter, Beta =\", noise_beta, epoch)\n",
    "                noise_samples = torch.tensor(data_dictionary[noise_beta]).to(device)\n",
    "            \n",
    "                for index in range(0, 200, 1):\n",
    "                    train_helper(model, optimizer, noise_samples[index*150:(index+1)*150], \n",
    "                        true_samples[index*batchsize:(index+1)*batchsize], btrue, noise_beta)\n",
    "\n",
    "btrue = 0.05\n",
    "train_main(modelA, optimizerA, betas, data_dictionaryA, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model on Region B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter, Beta = 0.001 0\n",
      "Iter, Beta = 0.0064444444444444445 0\n",
      "Iter, Beta = 0.01188888888888889 0\n",
      "Iter, Beta = 0.017333333333333333 0\n",
      "Iter, Beta = 0.001 1\n",
      "Iter, Beta = 0.0064444444444444445 1\n",
      "Iter, Beta = 0.01188888888888889 1\n",
      "Iter, Beta = 0.017333333333333333 1\n",
      "Iter, Beta = 0.001 2\n",
      "Iter, Beta = 0.0064444444444444445 2\n",
      "Iter, Beta = 0.01188888888888889 2\n",
      "Iter, Beta = 0.017333333333333333 2\n",
      "Iter, Beta = 0.001 3\n",
      "Iter, Beta = 0.0064444444444444445 3\n",
      "Iter, Beta = 0.01188888888888889 3\n",
      "Iter, Beta = 0.017333333333333333 3\n",
      "Iter, Beta = 0.001 4\n",
      "Iter, Beta = 0.0064444444444444445 4\n",
      "Iter, Beta = 0.01188888888888889 4\n",
      "Iter, Beta = 0.017333333333333333 4\n",
      "Iter, Beta = 0.001 5\n",
      "Iter, Beta = 0.0064444444444444445 5\n",
      "Iter, Beta = 0.01188888888888889 5\n",
      "Iter, Beta = 0.017333333333333333 5\n",
      "Iter, Beta = 0.001 6\n",
      "Iter, Beta = 0.0064444444444444445 6\n",
      "Iter, Beta = 0.01188888888888889 6\n",
      "Iter, Beta = 0.017333333333333333 6\n",
      "Iter, Beta = 0.001 7\n",
      "Iter, Beta = 0.0064444444444444445 7\n",
      "Iter, Beta = 0.01188888888888889 7\n",
      "Iter, Beta = 0.017333333333333333 7\n",
      "Iter, Beta = 0.001 8\n",
      "Iter, Beta = 0.0064444444444444445 8\n",
      "Iter, Beta = 0.01188888888888889 8\n",
      "Iter, Beta = 0.017333333333333333 8\n",
      "Iter, Beta = 0.001 9\n",
      "Iter, Beta = 0.0064444444444444445 9\n",
      "Iter, Beta = 0.01188888888888889 9\n",
      "Iter, Beta = 0.017333333333333333 9\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "modelB = NCE().to(device)\n",
    "optimizerB = optim.Adam(modelB.parameters(), lr=0.5*1e-3)\n",
    "btrue = 0.05\n",
    "train_main(modelB, optimizerB, betas, data_dictionaryB, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display The Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scatter() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-651819f48dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamplesA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: scatter() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "plt.scatter(samplesA[0], samplesA[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The free energy difference should be: -28.46."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-282.3481)\n",
      "tensor(-279.8099)\n",
      "tensor(-2.5383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-114-56cf5d4a23c9>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return (modelA(torch.tensor(samples).reshape(len(samples), 2).to(device))).cpu().detach().numpy()\n",
      "<ipython-input-114-56cf5d4a23c9>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return (modelB(torch.tensor(samples).reshape(len(samples), 2).to(device))).cpu().detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "def E_A(samples):\n",
    "    return (modelA(torch.tensor(samples).reshape(len(samples), 2).to(device))).cpu().detach().numpy()\n",
    "\n",
    "def E_B(samples):\n",
    "    return (modelB(torch.tensor(samples).reshape(len(samples), 2).to(device))).cpu().detach().numpy()\n",
    "\n",
    "samplesA = torch.tensor(data_dictionaryA[0.05])[0:10]\n",
    "samplesB = torch.tensor(data_dictionaryB[0.05])[0:10]\n",
    "\n",
    "F_A = (-1/beta)*np.log(\n",
    "    np.sum(\n",
    "        np.exp( E_A(samplesA) - np.array(compute_Muller_potential(0.05, samplesA)) )\n",
    "        )\n",
    "    )\n",
    "\n",
    "F_B = (-1/beta)*np.log(\n",
    "    np.sum(\n",
    "        np.exp( E_B(samplesB) - np.array(compute_Muller_potential(0.05, samplesB)) )\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(F_A)\n",
    "print(F_B)\n",
    "print(F_A - F_B)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bdb4c9c3f61b306428a8735aa29fdb3d47c708c66b6a0572dccc6544bde0e67"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('MMCD': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
